{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1,2'\n",
    "#import wget\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import argparse\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from functools import partial\n",
    "import re\n",
    "from  tqdm import tqdm\n",
    "import torch\n",
    "nltk.download('punkt')\n",
    "from src.dataset_classes import DatasetObject,Features,SquadQuestionGenerationDataset\n",
    "from src.utils import answerGeneratorDataset,questionGeneratorDataset,buildFact,setuptokenizer,pad_seq,SmartCollator,process_extra\n",
    "from dataclasses import dataclass, field\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_train = pd.read_csv('../datasets/processed_new_data.csv').dropna()\n",
    "squad_train = pd.read_csv('../datasets/train-v2.0.csv').dropna()\n",
    "squad_test = pd.read_csv('../datasets/test-v2.0.csv').dropna()\n",
    "\n",
    "train_raw_data = squad_train[['question', 'is_impossible', 'title', 'context', 'answer',\n",
    "       'answer_start', 'answer_end']]\n",
    "test_raw_data = squad_test[['question', 'is_impossible', 'title', 'context', 'answer',\n",
    "       'answer_start', 'answer_end']]#.sample(5000).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130315, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80000it [00:04, 19423.01it/s]\n",
      "80000it [00:08, 9790.33it/s] \n",
      "129it [00:00, 678026.59it/s]\n",
      "6000it [00:00, 18494.65it/s]\n",
      "5000it [00:00, 9392.38it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data_packet = questionGeneratorDataset(train_raw_data,80000) + answerGeneratorDataset(train_raw_data,80000) + process_extra(extra_train)\n",
    "random.shuffle(train_data_packet)\n",
    "random.shuffle(train_data_packet)\n",
    "\n",
    "test_data_packet = questionGeneratorDataset(test_raw_data,6000) + answerGeneratorDataset(test_raw_data,5000)\n",
    "random.shuffle(test_data_packet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = 'facebook/bart-base'\n",
    "tokenizer = setuptokenizer(model_base=model_base,\n",
    "                           special_tokens=['<section>','</section>'\n",
    "                                           ,'<generate_questions>',\n",
    "                                           '<generate_answers>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose the datasets\n",
    "train_dataset = SquadQuestionGenerationDataset(tokenizer=tokenizer,nb_records=len(train_raw_data),highlight_section=False)\n",
    "train_dataset.change_data_mode(1)\n",
    "train_dataset.set_record(train_data_packet)\n",
    "\n",
    "test_dataset = SquadQuestionGenerationDataset(tokenizer=tokenizer,nb_records=len(train_raw_data),highlight_section=False)\n",
    "test_dataset.change_data_mode(1)\n",
    "test_dataset.set_record(test_data_packet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159917"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration\n",
    "# Set up the model\n",
    "def model_init(device =  torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu') ):\n",
    "    generator = BartForConditionalGeneration.from_pretrained(model_base)\n",
    "    # update the tokens \n",
    "    generator.resize_token_embeddings(len(tokenizer))\n",
    "    return generator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional,Union,Callable,Dict,List,Tuple\n",
    "from transformers import TrainingArguments,Trainer,PreTrainedModel,DataCollator,PreTrainedTokenizerBase,EvalPrediction,EarlyStoppingCallback,TrainerCallback,ProgressCallback\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "class CustomTrainer(Trainer):\n",
    "  def __init__(self,device= torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu'), model: Union[PreTrainedModel, nn.Module] = None, args: TrainingArguments = None, data_collator: Optional[DataCollator] = None, train_dataset: Optional[Dataset] = None, eval_dataset: Optional[Dataset] = None, tokenizer: Optional[PreTrainedTokenizerBase] = None, model_init: Callable[[], PreTrainedModel] = None, compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None, callbacks: Optional[List[TrainerCallback]] = None, optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None,None), preprocess_logits_for_metrics: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = None):\n",
    "    super().__init__(model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\n",
    "    self.device = device\n",
    "  def compute_loss(self, model, batch, return_outputs=False):\n",
    "    \n",
    "    b_input_ids = batch['input_ids'].to(self.device)\n",
    "    b_input_mask = batch['attention_mask'].to(self.device)\n",
    "    b_labels =batch['labels'].to(self.device)\n",
    "    decoder_attention_mask = batch['decoder_attention_mask'].to(self.device)\n",
    "\n",
    "    #print(b_input_ids.shape)\n",
    "    outputs = model(b_input_ids, attention_mask = b_input_mask, decoder_attention_mask = decoder_attention_mask,\n",
    "                             labels = b_labels)\n",
    "    loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "    return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(overwrite_output_dir=True, output_dir='trained_models/setup_1/',\n",
    "                               evaluation_strategy='steps',\n",
    "                               lr_scheduler_type='cosine',\n",
    "                               adafactor=False,\n",
    "                               load_best_model_at_end=True,\n",
    "                               save_total_limit=1,\n",
    "                               weight_decay=0.3,\n",
    "                               warmup_ratio=0.21,\n",
    "                               num_train_epochs=5,\n",
    "                               per_device_train_batch_size=16,\n",
    "\n",
    "                               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_trainer = CustomTrainer(model_init= model_init,args=train_args, train_dataset=train_dataset, eval_dataset=test_dataset, \n",
    "                               data_collator=SmartCollator(\n",
    "                            pad_token_id=train_dataset.tokenizer.pad_token_id),callbacks=[EarlyStoppingCallback(early_stopping_patience=4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_trainer.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/nlplab/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/nlplab/.cache/huggingface/hub/models--facebook--bart-base/snapshots/84358834e73de6a82c22cec1d90eb45ef4f6eba5/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "/home/nlplab/anaconda3/envs/development/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 80091\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12515\n",
      "/home/nlplab/anaconda3/envs/development/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9500' max='12515' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 9500/12515 1:58:36 < 37:38, 1.33 it/s, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.012100</td>\n",
       "      <td>0.699232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.701600</td>\n",
       "      <td>0.658765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.661200</td>\n",
       "      <td>0.650026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.643800</td>\n",
       "      <td>0.642064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.638500</td>\n",
       "      <td>0.642977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.576400</td>\n",
       "      <td>0.635159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.583500</td>\n",
       "      <td>0.636798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.556200</td>\n",
       "      <td>0.632436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.575400</td>\n",
       "      <td>0.627813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.558000</td>\n",
       "      <td>0.628130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.470800</td>\n",
       "      <td>0.640605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.473900</td>\n",
       "      <td>0.633223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.472300</td>\n",
       "      <td>0.627176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.469700</td>\n",
       "      <td>0.624533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.470200</td>\n",
       "      <td>0.618976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.393500</td>\n",
       "      <td>0.638383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.387400</td>\n",
       "      <td>0.635146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.390500</td>\n",
       "      <td>0.634598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.393100</td>\n",
       "      <td>0.629989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 9982\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to trained_models/setup_1/checkpoint-500\n",
      "Configuration saved in trained_models/setup_1/checkpoint-500/config.json\n",
      "Model weights saved in trained_models/setup_1/checkpoint-500/pytorch_model.bin\n",
      "/home/nlplab/anaconda3/envs/development/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9982\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to trained_models/setup_1/checkpoint-1000\n",
      "Configuration saved in trained_models/setup_1/checkpoint-1000/config.json\n",
      "Model weights saved in trained_models/setup_1/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [trained_models/setup_1/checkpoint-500] due to args.save_total_limit\n",
      "/home/nlplab/anaconda3/envs/development/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9982\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to trained_models/setup_1/checkpoint-1500\n",
      "Configuration saved in trained_models/setup_1/checkpoint-1500/config.json\n",
      "Model weights saved in trained_models/setup_1/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [trained_models/setup_1/checkpoint-1000] due to args.save_total_limit\n",
      "/home/nlplab/anaconda3/envs/development/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9982\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to trained_models/setup_1/checkpoint-2000\n",
      "Configuration saved in trained_models/setup_1/checkpoint-2000/config.json\n",
      "Model weights saved in trained_models/setup_1/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [trained_models/setup_1/checkpoint-1500] due to args.save_total_limit\n",
      "/home/nlplab/anaconda3/envs/development/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9982\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to trained_models/setup_1/checkpoint-2500\n",
      "Configuration saved in trained_models/setup_1/checkpoint-2500/config.json\n",
      "Model weights saved in trained_models/setup_1/checkpoint-2500/pytorch_model.bin\n",
      "/home/nlplab/anaconda3/envs/development/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9982\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to trained_models/setup_1/checkpoint-3000\n",
      "Configuration saved in trained_models/setup_1/checkpoint-3000/config.json\n",
      "Model weights saved in trained_models/setup_1/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [trained_models/setup_1/checkpoint-2000] due to args.save_total_limit\n",
      "Deleting older checkpoint [trained_models/setup_1/checkpoint-2500] due to args.save_total_limit\n",
      "/home/nlplab/anaconda3/envs/development/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9982\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to trained_models/setup_1/checkpoint-3500\n",
      "Configuration saved in trained_models/setup_1/checkpoint-3500/config.json\n",
      "Model weights saved in trained_models/setup_1/checkpoint-3500/pytorch_model.bin\n",
      "/home/nlplab/anaconda3/envs/development/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9982\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to trained_models/setup_1/checkpoint-4000\n",
      "Configuration saved in trained_models/setup_1/checkpoint-4000/config.json\n",
      "Model weights saved in trained_models/setup_1/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [trained_models/setup_1/checkpoint-3000] due to args.save_total_limit\n",
      "Deleting older checkpoint [trained_models/setup_1/checkpoint-3500] due to args.save_total_limit\n",
      "/home/nlplab/anaconda3/envs/development/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9982\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to trained_models/setup_1/checkpoint-4500\n",
      "Configuration saved in trained_models/setup_1/checkpoint-4500/config.json\n",
      "Model weights saved in trained_models/setup_1/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [trained_models/setup_1/checkpoint-4000] due to args.save_total_limit\n",
      "/home/nlplab/anaconda3/envs/development/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9982\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to trained_models/setup_1/checkpoint-5000\n",
      "Configuration saved in trained_models/setup_1/checkpoint-5000/config.json\n",
      "Model weights saved in trained_models/setup_1/checkpoint-5000/pytorch_model.bin\n",
      "/home/nlplab/anaconda3/envs/development/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9982\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to trained_models/setup_1/checkpoint-5500\n",
      "Configuration saved in trained_models/setup_1/checkpoint-5500/config.json\n",
      "Model weights saved in trained_models/setup_1/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [trained_models/setup_1/checkpoint-5000] due to args.save_total_limit\n",
      "/home/nlplab/anaconda3/envs/development/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9982\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to trained_models/setup_1/checkpoint-6000\n",
      "Configuration saved in trained_models/setup_1/checkpoint-6000/config.json\n",
      "Model weights saved in trained_models/setup_1/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [trained_models/setup_1/checkpoint-5500] due to args.save_total_limit\n",
      "/home/nlplab/anaconda3/envs/development/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9982\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to trained_models/setup_1/checkpoint-6500\n",
      "Configuration saved in trained_models/setup_1/checkpoint-6500/config.json\n",
      "Model weights saved in trained_models/setup_1/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [trained_models/setup_1/checkpoint-4500] due to args.save_total_limit\n",
      "Deleting older checkpoint [trained_models/setup_1/checkpoint-6000] due to args.save_total_limit\n",
      "/home/nlplab/anaconda3/envs/development/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9982\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to trained_models/setup_1/checkpoint-7000\n",
      "Configuration saved in trained_models/setup_1/checkpoint-7000/config.json\n",
      "Model weights saved in trained_models/setup_1/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [trained_models/setup_1/checkpoint-6500] due to args.save_total_limit\n",
      "/home/nlplab/anaconda3/envs/development/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9982\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to trained_models/setup_1/checkpoint-7500\n",
      "Configuration saved in trained_models/setup_1/checkpoint-7500/config.json\n",
      "Model weights saved in trained_models/setup_1/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [trained_models/setup_1/checkpoint-7000] due to args.save_total_limit\n",
      "/home/nlplab/anaconda3/envs/development/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9982\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to trained_models/setup_1/checkpoint-8000\n",
      "Configuration saved in trained_models/setup_1/checkpoint-8000/config.json\n",
      "Model weights saved in trained_models/setup_1/checkpoint-8000/pytorch_model.bin\n",
      "/home/nlplab/anaconda3/envs/development/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9982\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to trained_models/setup_1/checkpoint-8500\n",
      "Configuration saved in trained_models/setup_1/checkpoint-8500/config.json\n",
      "Model weights saved in trained_models/setup_1/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [trained_models/setup_1/checkpoint-8000] due to args.save_total_limit\n",
      "/home/nlplab/anaconda3/envs/development/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9982\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to trained_models/setup_1/checkpoint-9000\n",
      "Configuration saved in trained_models/setup_1/checkpoint-9000/config.json\n",
      "Model weights saved in trained_models/setup_1/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [trained_models/setup_1/checkpoint-8500] due to args.save_total_limit\n",
      "/home/nlplab/anaconda3/envs/development/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9982\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to trained_models/setup_1/checkpoint-9500\n",
      "Configuration saved in trained_models/setup_1/checkpoint-9500/config.json\n",
      "Model weights saved in trained_models/setup_1/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [trained_models/setup_1/checkpoint-9000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from trained_models/setup_1/checkpoint-7500 (score: 0.6189760565757751).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9500, training_loss=0.5488569207442434, metrics={'train_runtime': 7117.8339, 'train_samples_per_second': 56.261, 'train_steps_per_second': 1.758, 'total_flos': 5.898681838393344e+16, 'train_loss': 0.5488569207442434, 'epoch': 3.8})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "import wikipedia\n",
    "def factgenerator(document,n):\n",
    "    return list(ngrams(sent_tokenize(document.strip()),n))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = wikipedia.summary('pan cake')\n",
    "n=3\n",
    "facts = [' '.join(s).replace('\\n','').strip() for s in factgenerator(article,n=n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A pancake (or hot-cake, griddlecake, or flapjack) is a flat cake, often thin and round, prepared from a starch-based batter that may contain eggs, milk and butter and cooked on a hot surface such as a griddle or frying pan, often frying with oil or butter. It is a type of batter bread. Archaeological evidence suggests that pancakes were probably eaten in prehistoric societies.The pancake\\'s shape and structure varies worldwide. In the United Kingdom, pancakes are often unleavened and resemble a crêpe. In North America, a leavening agent is used (typically baking powder) creating a thick fluffy pancake. A crêpe is a thin Breton pancake of French origin cooked on one or both sides in a special pan or crepe maker to achieve a lacelike network of fine bubbles. A well-known variation originating from southeast Europe is a palačinke, a thin moist pancake fried on both sides and filled with jam, cream cheese, chocolate, or ground walnuts, but many other fillings—sweet or savoury—can also be used.\\nCommercially prepared pancake mixes are available in some countries. Like waffles, commercially prepared frozen pancakes are available from companies like Eggo. When buttermilk is used in place of or in addition to milk, the pancake develops a tart flavor and becomes known as a buttermilk pancake, which is common in Scotland and the US. Buckwheat flour can be used in a pancake batter, making for a type of buckwheat pancake, a category that includes Blini, Kaletez, Ploye, and Memil-buchimgae. When potato is used as a major portion of the batter, the result is a potato pancake.  \\nPancakes may be served at any time of the day or year with a variety of toppings or fillings, but they have developed associations with particular times and toppings in different regions. In North America, they are typically considered a breakfast food and serve a similar function to waffles.  In Britain and the Commonwealth, they are associated with Shrove Tuesday, commonly known as \"Pancake Day\", when, historically, perishable ingredients had to be used up before the fasting period of Lent.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_too = True\n",
    "sampling_helper = {} if not sample_too else dict(top_k=30, top_p=0.95,)\n",
    "max_length=250\n",
    "length_penalty=2.6\n",
    "beam_size=4\n",
    "repetition_penalty=1.56\n",
    "return_top_beams= beam_size if not sample_too else 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SquadQuestionGenerationDataset(tokenizer,nb_records=1)\n",
    "dataset.change_data_mode(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = custom_trainer.model\n",
    "device = custom_trainer.model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Section: Buckwheat flour can be used in a pancake batter, making for a type of buckwheat pancake, a category that includes Blini, Kaletez, Ploye, and Memil-buchimgae. When potato is used as a major portion of the batter, the result is a potato pancake. Pancakes may be served at any time of the day or year with a variety of toppings or fillings, but they have developed associations with particular times and toppings in different regions.\n",
      "Questions Generated\n",
      "What is the result of a potato pancake when potato is used as a major part of the batter?\n",
      "What is the result of a potato pancake when used as a large part of the batter?\n",
      "What is the result of a potato pancake when potato is used as a major part of the batter?\n",
      "What is the result of a potato pancake when potato is used as a major part of the batter?\n",
      "What is the result of a potato pancake when used as a large part of the batter?\n",
      "What is the result of a potato pancake when potato is used as a major part of the batter?\n",
      "What is the result of a potato pancake when potato is used as a major part of the batter?\n",
      "What is the result of a potato pancake when potato is used?\n",
      "What is the result of a potato pancake when potato is used?\n",
      "What is the result of a potato pancake when potato is used?\n"
     ]
    }
   ],
   "source": [
    "task_id = 0\n",
    "target_fact = facts[10]\n",
    "#' item[COA], volume[32m3],  ratio[0.06]  Table 1: Chemical combination rule for working with N2 gas.'\n",
    "data = DatasetObject(task='<generate_questions> ', question='', context=target_fact ,fact=target_fact ,answer='',answer_sentence='',task_id=\"\")\n",
    "\n",
    "batch = dataset.procesTexts(data)\n",
    "\n",
    "b_input_ids = batch.input_ids.view(1,-1).to(device)\n",
    "b_input_mask = batch.attention_mask.view(1,-1).to(device)\n",
    "\n",
    "sample_too = True\n",
    "sampling_helper = {} if not sample_too else dict(top_k=25, top_p=0.95,)\n",
    "return_top_beams= beam_size if not sample_too else 10\n",
    "#seed_everything(2982)\n",
    "with torch.no_grad():\n",
    "  sample_outputs = generator.generate(input_ids=b_input_ids,  **sampling_helper,\n",
    "                                                 attention_mask=b_input_mask ,\n",
    "                                                 num_beams=beam_size,\n",
    "                                                 repetition_penalty=repetition_penalty,\n",
    "                                                 length_penalty=length_penalty,\n",
    "                                                 early_stopping=True,\n",
    "                                                 use_cache=True,\n",
    "                                                 max_length=max_length,\n",
    "                                                 no_repeat_ngram_size=2,\n",
    "                                                 num_return_sequences=return_top_beams,\n",
    "                                                 do_sample=sample_too,\n",
    "                                                 eos_token_id=dataset.tokenizer.eos_token_id,)\n",
    "oop = [dataset.tokenizer.decode(sample_outputs[idx],\n",
    "                                     skip_special_tokens=True,\n",
    "                                     clean_up_tokenization_spaces=True) for idx in range(return_top_beams)]\n",
    "\n",
    "print(f'Article Section: {data.context}')\n",
    "print('Questions Generated')\n",
    "for q in oop:\n",
    "  print(q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('development')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "473435c5caf2da67d3d84349b3ab99ae605588908510e1f3cdf041055f6c21f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
