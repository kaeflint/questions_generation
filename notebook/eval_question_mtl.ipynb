{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "#import wget\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import argparse\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from functools import partial\n",
    "import re\n",
    "from  tqdm import tqdm\n",
    "import torch\n",
    "nltk.download('punkt')\n",
    "from functools import partial\n",
    "import nltk\n",
    "from src.dataset_processor import load_all_data\n",
    "from src.utils import SmartCollator, get_args, setuptokenizer\n",
    "from src.dataset_processor import (\n",
    "    Multi_taskQuestionGenerationDataset as QuestionGenerationDataset,\n",
    ")\n",
    "from src.model_utils import CustomTrainer, get_training_arguments, model_init\n",
    "from src.config import DATASET_PATH, GenerationTasks\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlplab/anaconda3/envs/development/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_base = 't5-base'\n",
    "tokenizer = setuptokenizer(model_base=model_base,special_tokens=[\n",
    "            GenerationTasks.vanilla_question_gen,\n",
    "            GenerationTasks.context_question_gen,\n",
    "            GenerationTasks.question_paraphrase,\n",
    "            \"<section>\",\n",
    "            \"</section>\",\n",
    "        ],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset_processor import Multi_taskQuestionGenerationDataset\n",
    "dataset = Multi_taskQuestionGenerationDataset(tokenizer,nb_records=1)\n",
    "dataset.change_data_mode(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model_utils import model_init\n",
    "saved_model_path = '../trained_models_mtl/t5_base_model_2/checkpoint-120790/'\n",
    "#'../trained_models_mtl/bart_base_model_1/checkpoint-25524//'\n",
    "trained_weights = torch.load(f'{saved_model_path}/pytorch_model.bin')\n",
    "\n",
    "generator = model_init(model_base=model_base,vocab_size=len(tokenizer))\n",
    "generator.load_state_dict(trained_weights)\n",
    "device = generator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_default_sentence_split\n",
    "default_sentence_split = get_default_sentence_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "import wikipedia\n",
    "def factgenerator(document,n):\n",
    "    return list(ngrams(default_sentence_split(document.strip()),n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_too = True\n",
    "sampling_helper = {} if not sample_too else dict(top_k=30, top_p=0.95,)\n",
    "max_length=80\n",
    "length_penalty=2.6\n",
    "beam_size=4\n",
    "repetition_penalty=1.56\n",
    "return_top_beams= beam_size if not sample_too else 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing files:  ['../curated_data/squad_dev.csv', '../curated_data/drop_dev.csv', '../curated_data/rope_dev.csv', '../curated_data/sci_dev.csv']\n",
      "processing files:  ['../curated_data/sci_test.csv']\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = '../curated_data/'\n",
    "#train_data_packet = load_all_data(DATASET_PATH, mode=\"train\")\n",
    "test_data_packet = load_all_data(DATASET_PATH, mode=\"dev\")+load_all_data(DATASET_PATH, mode=\"test\")\n",
    "test_dataset = QuestionGenerationDataset(\n",
    "        tokenizer=tokenizer, nb_records=len(test_data_packet), highlight_section=False\n",
    "    )\n",
    "test_dataset.change_data_mode(1)\n",
    "test_dataset.set_record(test_data_packet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43468"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,SequentialSampler, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToFile(content, filename):\n",
    "    fil = filename+'.txt'\n",
    "    if os.path.exists(fil):\n",
    "        os.remove(fil)\n",
    "    with open(fil, 'x') as fwrite:\n",
    "        fwrite.writelines(\"%s\\n\" % s for s in content)\n",
    "    print('Done')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateOutput(dataset: Dataset,beam_size = 4,batch_size=20):\n",
    "    dataset_loader = DataLoader(dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              collate_fn= SmartCollator(tokenizer.pad_token_id,is_inference= True,),\n",
    "                              sampler=SequentialSampler(dataset))\n",
    "    generated_texts = []\n",
    "    for batch in tqdm(dataset_loader):\n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_input_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        sample_too =  False\n",
    "        sampling_helper = {} if not sample_too else dict(top_k=25, top_p=0.95,)\n",
    "        return_top_beams = 1 if not sample_too else 25\n",
    "        # seed_everything(2982)\n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            sample_outputs = generator.generate(input_ids=b_input_ids,  **sampling_helper,\n",
    "                                                attention_mask=b_input_mask,\n",
    "                                                num_beams=beam_size,\n",
    "                                                repetition_penalty=repetition_penalty,\n",
    "                                                length_penalty=length_penalty,\n",
    "                                                early_stopping=False,\n",
    "                                                use_cache=True,\n",
    "                                                max_length=max_length,\n",
    "                                                no_repeat_ngram_size=2,\n",
    "                                                num_return_sequences=return_top_beams,\n",
    "                                                do_sample=sample_too,\n",
    "                                                eos_token_id=dataset.tokenizer.eos_token_id,)\n",
    "        oop = [dataset.tokenizer.decode(s,\n",
    "                                        skip_special_tokens=True,\n",
    "                                        clean_up_tokenization_spaces=True) for s in sample_outputs ]\n",
    "        generated_texts+=oop\n",
    "    return generated_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/2174 [00:06<37:48,  1.05s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (639 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 2174/2174 [39:23<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "reference_sentences =[d.output_text for d in test_data_packet]\n",
    "generated_text = generateOutput(test_dataset,batch_size=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "writeToFile(generated_text,saved_model_path+'beam_size4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = evaluate.load('bertscore',lang='en',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "scorer=evaluate.combine(['bleu',\n",
    "                         'meteor',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.15058681184538658,\n",
       " 'precisions': [0.35794498467860497,\n",
       "  0.16658570130722253,\n",
       "  0.1089699736515914,\n",
       "  0.07913833443006507],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.3022202659090865,\n",
       " 'translation_length': 677484,\n",
       " 'reference_length': 520253,\n",
       " 'meteor': 0.40187798663956564}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer.compute(predictions=generated_text,\n",
    "               references=reference_sentences,lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bscores=bertscore.compute(predictions=generated_text,\n",
    "               references=reference_sentences,lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9056642513708126, dict_keys(['precision', 'recall', 'f1', 'hashcode']))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(bscores['f1']),bscores.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readSentences(file,lower=False):\n",
    "    with open(file,'r', encoding=\"utf-8\") as o_file:\n",
    "        sentennces = []\n",
    "        for s in o_file.readlines():\n",
    "            ss = s.strip() #.lower() if  lower else s.strip()\n",
    "            sentennces.append(ss)\n",
    "    return sentennces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mut_gens = readSentences(saved_model_path+'/beam_4.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_ref = []\n",
    "for s in reference_sentences:\n",
    "    expanded_ref +=[s]*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "bscores=bertscore.compute(predictions=mut_gens,\n",
    "               references=expanded_ref,lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9111872966340173"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(bscores['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.14399644886631266,\n",
       " 'precisions': [0.35924705974114024,\n",
       "  0.16233583376790084,\n",
       "  0.10248129819907306,\n",
       "  0.07193742552989153],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.2814784345308916,\n",
       " 'translation_length': 2666772,\n",
       " 'reference_length': 2081012,\n",
       " 'meteor': 0.3933352651188569}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(expanded_ref),len(mut_gens)\n",
    "scorer.compute(predictions=mut_gens,\n",
    "               references=expanded_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = wikipedia.summary(\"Earth crust\",auto_suggest=False)\n",
    "\n",
    "\"\"\"\n",
    "Metals represent approximately 25% of the elemental makeup of the Earth's crust. \n",
    "The bulk of these metals, primarily aluminum, iron, calcium, sodium, potassium, and magnesium, are typically found in combined form. The most abundant metal is aluminum, which occurs almost exclusively as the ionic mineral bauxite. The other most common metals, including iron, sodium, potassium, magnesium, and calcium, are also found primarily as the cationic portion of an ionic compound. Very few metals actually occur naturally as pure substances.\n",
    "The ones that do are often referred to as precious or semi-precious metals.\n",
    "\"\"\"\n",
    "#wikipedia.summary('Dragon')\n",
    "n=3\n",
    "facts = [' '.join(s).replace('\\n','').strip() for s in factgenerator(article.replace('.T','. T'),n=n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The lithosphere is broken into tectonic plates whose motion allows heat to escape from the interior of the Earth into space. The crust lies on top of the mantle, a configuration that is stable because the upper mantle is made of peridotite and is therefore significantly denser than the crust. The boundary between the crust and mantle is conventionally placed at the Mohorovičić discontinuity, a boundary defined by a contrast in seismic velocity.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Section: Earth's crust is Earth's thin outer shell of rock, regarding for less than 1% of Earth's radius and volume. It is the top component of the lithosphere, a division of Earth's layers that includes the crust and the upper part of the mantle. The lithosphere is broken into tectonic plates whose motion allows heat to escape from the interior of the Earth into space.\n",
      "Questions Generated\n",
      "What is the top component of the earth's lithosphere?\n",
      "What is the top component of the lithosphere?\n"
     ]
    }
   ],
   "source": [
    "from src.dataset_processor import QuestionGenerationData\n",
    "\n",
    "\n",
    "task_id = 0\n",
    "target_fact = facts[0]\n",
    "#facts[-1]\n",
    "#'Table 1 Chemical combination rule for working with N2 gas: item[COA]. volume[32m3].  ratio[0.06].  '\n",
    "#facts[11]\n",
    "\n",
    "#' item[COA], volume[32m3],  ratio[0.06]  Table 1: Chemical combination rule for working with N2 gas.'\n",
    "data =  QuestionGenerationData(task=GenerationTasks.vanilla_question_gen,\n",
    "                                            input_text= target_fact, \n",
    "                                            output_text='',\n",
    "                                            contextual_text= '')\n",
    "\n",
    "batch = dataset.procesTexts(data)\n",
    "\n",
    "b_input_ids = batch.input_ids.view(1, -1).to(device)\n",
    "b_input_mask = batch.attention_mask.view(1, -1).to(device)\n",
    "\n",
    "sample_too =  True\n",
    "sampling_helper = {} if not sample_too else dict(top_k=25, top_p=0.95,)\n",
    "return_top_beams = beam_size if not sample_too else 25\n",
    "# seed_everything(2982)\n",
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    sample_outputs = generator.generate(input_ids=b_input_ids,  **sampling_helper,\n",
    "                                        attention_mask=b_input_mask,\n",
    "                                        num_beams=beam_size,\n",
    "                                        repetition_penalty=repetition_penalty,\n",
    "                                        length_penalty=length_penalty,\n",
    "                                        early_stopping=False,\n",
    "                                        use_cache=True,\n",
    "                                        max_length=max_length,\n",
    "                                        no_repeat_ngram_size=2,\n",
    "                                        num_return_sequences=return_top_beams,\n",
    "                                        do_sample=sample_too,\n",
    "                                        eos_token_id=dataset.tokenizer.eos_token_id,)\n",
    "oop = [dataset.tokenizer.decode(sample_outputs[idx],\n",
    "                                skip_special_tokens=True,\n",
    "                                clean_up_tokenization_spaces=True) for idx in range(return_top_beams)]\n",
    "\n",
    "print(f'Article Section: {data.input_text}')\n",
    "print('Questions Generated')\n",
    "oop = set(oop)\n",
    "for q in oop:\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Section: Metals represent approximately 25% of the elemental makeup of the Earth's crust. The bulk of these metals, primarily aluminum, iron, calcium, sodium, potassium, and magnesium, are typically found in combined form. The most abundant metal is aluminum, which occurs almost exclusively as the ionic mineral bauxite. The other most common metals, including iron, sodium, potassium, magnesium, and calcium, are also found primarily as the cationic portion of an ionic compound. Very few metals actually occur naturally as pure substances. The ones that do are often referred to as precious or semi-precious metals.\n",
      "Questions Generated\n",
      "What is the most abundant metal that occurs almost directly as the ionic mineral bauxite?\n",
      "What is the most abundant metal that occurs almost exclusively as the ionic mineral bauxite?\n",
      "What is the most abundant metal in the earth's crust?\n",
      "What is the most abundant metal?\n",
      "What is the most abundant metal of the earth's crust, which occurs almost exclusively as the ionic mineral bauxite?\n"
     ]
    }
   ],
   "source": [
    "What is the earth's thin outer shell of rock?\n",
    "What is the earth's thin outer shell of rock known as?\n",
    "What is the top component of the lithosphere?\n",
    "What is the earth's thin outer shell of rock referred to as?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Section: What is the name of a reptile-like legendary creature that appears in the folklore of many cultures worldwide?\n",
      "Questions Generated\n",
      "What is the name of a reptile-like legendary creature that appears in the folklore of many cultures worldwide?\n"
     ]
    }
   ],
   "source": [
    "from src.dataset_processor import QuestionGenerationData\n",
    "\n",
    "\n",
    "task_id = 0\n",
    "target_fact = \"What is the name of a reptile-like legendary creature that appears in the folklore of many cultures worldwide?\"\n",
    "#'Table 1 Chemical combination rule for working with N2 gas: item[COA]. volume[32m3].  ratio[0.06].  '\n",
    "#facts[11]\n",
    "\n",
    "#' item[COA], volume[32m3],  ratio[0.06]  Table 1: Chemical combination rule for working with N2 gas.'\n",
    "data =  QuestionGenerationData(task=GenerationTasks.question_paraphrase,\n",
    "                                            input_text= target_fact, \n",
    "                                            output_text='',\n",
    "                                            contextual_text= '')\n",
    "\n",
    "batch = dataset.procesTexts(data)\n",
    "\n",
    "b_input_ids = batch.input_ids.view(1, -1).to(device)\n",
    "b_input_mask = batch.attention_mask.view(1, -1).to(device)\n",
    "\n",
    "sample_too = True\n",
    "sampling_helper = {} if not sample_too else dict(top_k=25, top_p=0.95,)\n",
    "return_top_beams = beam_size if not sample_too else 25\n",
    "# seed_everything(2982)\n",
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    sample_outputs = generator.generate(input_ids=b_input_ids,  **sampling_helper,\n",
    "                                        attention_mask=b_input_mask,\n",
    "                                        num_beams=beam_size,\n",
    "                                        repetition_penalty=repetition_penalty,\n",
    "                                        length_penalty=length_penalty,\n",
    "                                        early_stopping=False,\n",
    "                                        use_cache=True,\n",
    "                                        max_length=max_length,\n",
    "                                        no_repeat_ngram_size=2,\n",
    "                                        num_return_sequences=return_top_beams,\n",
    "                                        do_sample=sample_too,\n",
    "                                        eos_token_id=dataset.tokenizer.eos_token_id,)\n",
    "oop = [dataset.tokenizer.decode(sample_outputs[idx],\n",
    "                                skip_special_tokens=True,\n",
    "                                clean_up_tokenization_spaces=True) for idx in range(return_top_beams)]\n",
    "\n",
    "print(f'Article Section: {data.input_text}')\n",
    "print('Questions Generated')\n",
    "oop = set(oop)\n",
    "for q in oop:\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '../curated_data/'\n",
    "train_data_packet = load_all_data(DATASET_PATH, mode=\"train\")\n",
    "test_data_packet = load_all_data(DATASET_PATH, mode=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing files:  ['../curated_data/drop_train.csv', '../curated_data/squad_train.csv', '../curated_data/rope_train.csv', '../curated_data/extra_data_train.csv', '../curated_data/sci_train.csv']\n",
      "processing files:  ['../curated_data/squad_dev.csv', '../curated_data/drop_dev.csv', '../curated_data/rope_dev.csv', '../curated_data/sci_dev.csv']\n"
     ]
    }
   ],
   "source": [
    "train_data_packet = load_all_data(DATASET_PATH, mode=\"train\")\n",
    "test_data_packet = load_all_data(DATASET_PATH, mode=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71196"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vv =[v for v in train_data_packet if 'How' in v.output_text]\n",
    "len(vv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408372"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_packet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('development')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "473435c5caf2da67d3d84349b3ab99ae605588908510e1f3cdf041055f6c21f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
