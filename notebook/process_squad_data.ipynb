{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "import random\n",
    "sys.path.append('../')\n",
    "from nltk.data import load\n",
    "tokenizer = load('tokenizers/punkt/{0}.pickle'.format('english'))\n",
    "tokenizer._params.abbrev_types.add('..')\n",
    "tokenizer._params.abbrev_types.add('No')\n",
    "tokenizer._params.abbrev_types.add('no')\n",
    "tokenizer._params.abbrev_types.add('Dr')\n",
    "tokenizer._params.abbrev_types.add('dr')\n",
    "tokenizer._params.abbrev_types.add('op')\n",
    "tokenizer._params.abbrev_types.add('J.S.')\n",
    "\n",
    "def default_sentence_split(passage):\n",
    "    return tokenizer.tokenize(passage)\n",
    "extra_train = pd.read_csv('../datasets/processed_new_data.csv',index_col=None,encoding='utf-8').dropna()\n",
    "squad_train = pd.read_csv('../datasets/train-v2.0.csv',index_col=None).dropna()\n",
    "squad_test = pd.read_csv('../datasets/test-v2.0.csv',index_col=None).dropna()\n",
    "\n",
    "train_raw_data = squad_train[['question', 'is_impossible', 'title', 'context', 'answer',\n",
    "                                  'answer_start', 'answer_end']]\n",
    "test_raw_data = squad_test[['question', 'is_impossible', 'title', 'context', 'answer',\n",
    "                                'answer_start', 'answer_end']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy library\n",
    "import spacy\n",
    "  \n",
    "#load core english library\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.tokenizer.add_special_case(\"No.\",[{\"ORTH\":\"No.\"}])\n",
    "nlp.tokenizer.add_special_case(\"Op.\",[{\"ORTH\":\"Op.\"}])\n",
    "nlp.tokenizer.add_special_case('..',[{\"ORTH\":\"..\"}])\n",
    "nlp.tokenizer.add_special_case('No',[{\"ORTH\":\"No\"}])\n",
    "nlp.tokenizer.add_special_case('no',[{\"ORTH\":\"no\"}])\n",
    "nlp.tokenizer.add_special_case('Dr.',[{\"ORTH\":\"Dr.\"}])\n",
    "nlp.tokenizer.add_special_case('dr.',[{\"ORTH\":\"dr.\"}])\n",
    "nlp.tokenizer.add_special_case('J.S.',[{\"ORTH\":\"J.S.\"}])\n",
    "def spacy_sentence_tokenizer(passage):\n",
    "    doc = nlp(passage)\n",
    "    return [s.text for s in doc.sents]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def spacy_search(answer,passage,start_idx):\n",
    "    end_idx = start_idx + len(answer)\n",
    "    sentences = spacy_sentence_tokenizer(passage)\n",
    "    sentence_marked =[]\n",
    "    correct = \"\"\n",
    "    running_len = 0\n",
    "    for s in sentences:\n",
    "        ll= len(s)\n",
    "        start = running_len \n",
    "        end= len(s) + running_len\n",
    "        running_len = end+1\n",
    "\n",
    "        \n",
    "        if end_idx <= end and answer in s:\n",
    "            correct=s\n",
    "    return correct,sentences\n",
    "def extract_passage_sentence(answer,passage,start_idx):\n",
    "    #answer_section = default_sentence_split(answer)\n",
    "    #print(answer_section)\n",
    "    end_idx = start_idx + len(answer)\n",
    "    sentences = default_sentence_split(passage)\n",
    "    sentence_marked =[]\n",
    "    correct = \"\"\n",
    "    running_len = 0\n",
    "    for s in sentences:\n",
    "        ll= len(s)\n",
    "        start = running_len \n",
    "        end= len(s) + running_len\n",
    "        running_len = end+1\n",
    "        if end_idx <= end and answer in s:\n",
    "            correct=s\n",
    "    # set up a fall back\n",
    "    correct_ = correct\n",
    "    if correct =='':\n",
    "        correct_c,sentences = spacy_search(answer,passage,start_idx)\n",
    "        correct = correct_\n",
    "        #sentences = [s for s in spacy_search(answer,passage,start_idx) if answer in s]\n",
    "        #correct = random.choice(sentences) if len(sentences)>0 else ''\n",
    "    if correct_ =='':\n",
    "        #print(answer)\n",
    "        answer_section = default_sentence_split(answer)\n",
    "        #print(answer_section)\n",
    "        sentences_ = [s for s in sentences if any([a in s for a in answer_section] )]\n",
    "        correct =' '.join(sentences_)\n",
    "\n",
    "\n",
    "    return correct,sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from nltk.util import ngrams\n",
    "from src.dataset_processor import QuestionGenerationData\n",
    "from src.config import GenerationTasks\n",
    "def selectContext(articles, answer_index, n=4):\n",
    "    if not isinstance(answer_index,list):\n",
    "        answer_index = [answer_index]\n",
    "    vals = list(ngrams(range(len(articles)), n))\n",
    "    possible_contexts = []\n",
    "    for l in vals:\n",
    "        if all([a in l for a in answer_index]):\n",
    "            possible_contexts.append(l)\n",
    "    if len(possible_contexts)>0:\n",
    "        return random.choice(possible_contexts)\n",
    "    return None\n",
    "\n",
    "def processSquadData(datapack: pd.DataFrame,verbose=False):\n",
    "    datapack_strip = datapack.values\n",
    "    dataset = []\n",
    "    for idx, dat in tqdm.tqdm(enumerate(datapack_strip )) if verbose else enumerate(datapack_strip ):\n",
    "        answer_sentence,sentences = extract_passage_sentence(dat[4],dat[3],dat[5])\n",
    "        answer_sentences = default_sentence_split(answer_sentence)\n",
    "        answer_sentence_index = [idx for idx,s in enumerate(sentences) if s in answer_sentences]\n",
    "        selected_context_idxs = selectContext(sentences, answer_sentence_index , n=random.choice([4,3,4]))\n",
    "\n",
    "        answer_fact = dat[3]\n",
    "        if  selected_context_idxs is not None:\n",
    "            answer_fact =  ' '.join([sentences[s] for s in selected_context_idxs])\n",
    "        \n",
    "        vani_dob_q = QuestionGenerationData(task=GenerationTasks.vanilla_question_gen,\n",
    "                                            input_text= answer_fact, \n",
    "                                            output_text=dat[0],\n",
    "                                            contextual_text= answer_sentence.strip())\n",
    "        context_questions = QuestionGenerationData(task=GenerationTasks.context_question_gen,\n",
    "                                                   input_text= dat[3], \n",
    "                                                   output_text= dat[0],\n",
    "                                                   contextual_text= dat[4].strip())\n",
    "        dataset.append(context_questions)\n",
    "        dataset.append(vani_dob_q)\n",
    "    return dataset\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_test_extended = processSquadData(test_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "130315it [00:09, 13729.70it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_train_extended = processSquadData(train_raw_data,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclass_csv import DataclassReader, dateformat,DataclassWriter\n",
    "\n",
    "with open(\"../curated_data/squad_train.csv\", \"w\",encoding='utf-8') as f:\n",
    "    w = DataclassWriter(f, squad_train_extended, QuestionGenerationData)\n",
    "    w.write()\n",
    "\n",
    "with open(\"../curated_data/squad_dev.csv\", \"w\",encoding='utf-8') as f:\n",
    "    w = DataclassWriter(f, squad_test_extended, QuestionGenerationData)\n",
    "    w.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src.dataset_processor  import load_dataset\n",
    "def load_dataset(data_path: str):\n",
    "    pack =[]\n",
    "    with open(data_path,encoding='utf-8') as f:\n",
    "        dataset = DataclassReader(f, QuestionGenerationData)\n",
    "        for row in dataset:\n",
    "            pack.append(row)\n",
    "    return pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "yu= load_dataset(\"../curated_data/squad_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yu==squad_train_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_extra(extra_data):\n",
    "    data = extra_data.values\n",
    "    dataset = []\n",
    "    for idx, dat in tqdm.tqdm(enumerate(data)):  # type: ignore\n",
    "        fact, answer_sentence = dat[3], dat[2]\n",
    "\n",
    "        vani_dob_q = QuestionGenerationData(task=GenerationTasks.vanilla_question_gen,\n",
    "                                            input_text= fact, \n",
    "                                            output_text=dat[1].strip(),\n",
    "                                            contextual_text= answer_sentence.strip())\n",
    "        \n",
    "        dataset.append(vani_dob_q)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionGenerationData(task='<generate_questions>', input_text=\"Meta Platforms, Inc., doing business as Meta and formerly named Facebook, Inc., and TheFacebook, Inc., is an American multinational technology conglomerate based in Menlo Park, California. Meta is one of the world's most valuable companies. It is considered one of the Big Five American information technology companies, alongside Alphabet, Amazon, Apple, and Microsoft. Meta's products and services include Facebook, Messenger, Facebook Watch, and Meta Portal. It has also acquired Oculus, Giphy, Mapillary, Kustomer, Presize and has a 9.99% stake in Jio Platforms.\", output_text='According to the article which companies make up the top five in terms of technology?', contextual_text=\"Meta is one of the world's most valuable companies. It is considered one of the Big Five American information technology companies, alongside Alphabet, Amazon, Apple, and Microsoft.\")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_extra[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_extra = process_extra(extra_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../curated_data/extra_data.csv\", \"w\",encoding='utf-8') as f:\n",
    "    w = DataclassWriter(f, processed_extra, QuestionGenerationData)\n",
    "    w.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (index_label, row_series) in train_raw_data.iterrows():\n",
    "    #print(k)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = pd.DataFrame.from_dict(row_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    in the late 1990s\n",
       "Name: answer, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.loc['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\isaac.ampomah\\development\\question_generation_trainer\\questions_generation\\notebook\\process_squad_data.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/isaac.ampomah/development/question_generation_trainer/questions_generation/notebook/process_squad_data.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m squad_train_extended \u001b[39m=\u001b[39m processSquad(train_raw_data)\n",
      "\u001b[1;32mc:\\Users\\isaac.ampomah\\development\\question_generation_trainer\\questions_generation\\notebook\\process_squad_data.ipynb Cell 10\u001b[0m in \u001b[0;36mprocessSquad\u001b[1;34m(datapack)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/isaac.ampomah/development/question_generation_trainer/questions_generation/notebook/process_squad_data.ipynb#X55sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     example[\u001b[39m'\u001b[39m\u001b[39manswer_end\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m example[\u001b[39m'\u001b[39m\u001b[39manswer_start\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(example[\u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/isaac.ampomah/development/question_generation_trainer/questions_generation/notebook/process_squad_data.ipynb#X55sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     example[\u001b[39m'\u001b[39m\u001b[39manswer_sentence\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39mextract_passage_sentence(example[\u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m],example[\u001b[39m'\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m],example[\u001b[39m'\u001b[39m\u001b[39manswer_start\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/isaac.ampomah/development/question_generation_trainer/questions_generation/notebook/process_squad_data.ipynb#X55sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     examples \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat([examples,example])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/isaac.ampomah/development/question_generation_trainer/questions_generation/notebook/process_squad_data.ipynb#X55sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mreturn\u001b[39;00m examples\n",
      "File \u001b[1;32mc:\\Users\\isaac.ampomah\\Anaconda3\\envs\\vsme\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\isaac.ampomah\\Anaconda3\\envs\\vsme\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:381\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39mConcatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[39m1   3   4\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    368\u001b[0m op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[0;32m    369\u001b[0m     objs,\n\u001b[0;32m    370\u001b[0m     axis\u001b[39m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    378\u001b[0m     sort\u001b[39m=\u001b[39msort,\n\u001b[0;32m    379\u001b[0m )\n\u001b[1;32m--> 381\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mget_result()\n",
      "File \u001b[1;32mc:\\Users\\isaac.ampomah\\Anaconda3\\envs\\vsme\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:616\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    612\u001b[0m             indexers[ax] \u001b[39m=\u001b[39m obj_labels\u001b[39m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    614\u001b[0m     mgrs_indexers\u001b[39m.\u001b[39mappend((obj\u001b[39m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 616\u001b[0m new_data \u001b[39m=\u001b[39m concatenate_managers(\n\u001b[0;32m    617\u001b[0m     mgrs_indexers, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnew_axes, concat_axis\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbm_axis, copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy\n\u001b[0;32m    618\u001b[0m )\n\u001b[0;32m    619\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy:\n\u001b[0;32m    620\u001b[0m     new_data\u001b[39m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32mc:\\Users\\isaac.ampomah\\Anaconda3\\envs\\vsme\\lib\\site-packages\\pandas\\core\\internals\\concat.py:233\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    231\u001b[0m     fastpath \u001b[39m=\u001b[39m blk\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m values\u001b[39m.\u001b[39mdtype\n\u001b[0;32m    232\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 233\u001b[0m     values \u001b[39m=\u001b[39m _concatenate_join_units(join_units, concat_axis, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[0;32m    234\u001b[0m     fastpath \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[39mif\u001b[39;00m fastpath:\n",
      "File \u001b[1;32mc:\\Users\\isaac.ampomah\\Anaconda3\\envs\\vsme\\lib\\site-packages\\pandas\\core\\internals\\concat.py:577\u001b[0m, in \u001b[0;36m_concatenate_join_units\u001b[1;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[0;32m    574\u001b[0m     concat_values \u001b[39m=\u001b[39m ensure_block_shape(concat_values, \u001b[39m2\u001b[39m)\n\u001b[0;32m    576\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 577\u001b[0m     concat_values \u001b[39m=\u001b[39m concat_compat(to_concat, axis\u001b[39m=\u001b[39;49mconcat_axis)\n\u001b[0;32m    579\u001b[0m \u001b[39mreturn\u001b[39;00m concat_values\n",
      "File \u001b[1;32mc:\\Users\\isaac.ampomah\\Anaconda3\\envs\\vsme\\lib\\site-packages\\pandas\\core\\dtypes\\concat.py:151\u001b[0m, in \u001b[0;36mconcat_compat\u001b[1;34m(to_concat, axis, ea_compat_axis)\u001b[0m\n\u001b[0;32m    148\u001b[0m             to_concat \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m to_concat]\n\u001b[0;32m    149\u001b[0m             kinds \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mo\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m--> 151\u001b[0m result \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate(to_concat, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m    152\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kinds \u001b[39mand\u001b[39;00m result\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mu\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    153\u001b[0m     \u001b[39m# GH#39817\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    155\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBehavior when concatenating bool-dtype and numeric-dtype arrays is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdeprecated; in a future version these will cast to object dtype \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    161\u001b[0m     )\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "squad_train_extended = processSquad(train_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def extract_passage_sentence_debug(answer,passage,start_idx):\n",
    "    end_idx = start_idx + len(answer)\n",
    "    sentences = spacy_sentence_tokenizer(passage)\n",
    "    sentence_marked =[]\n",
    "    correct = \"\"\n",
    "    running_len = 0\n",
    "    for s in sentences:\n",
    "        ll= len(s)\n",
    "        start = running_len \n",
    "        end= len(s) + running_len\n",
    "        running_len = end+1\n",
    "\n",
    "        print(start,'Yes',end,end_idx)\n",
    "        if start_idx>= start:\n",
    "            #correct=s\n",
    "            print(start,'Yes',end,end_idx)\n",
    "            #print(start,s,end)\n",
    "        \n",
    "        elif answer in s:\n",
    "            print('Found')\n",
    "        if end_idx <= end and answer in s:\n",
    "            correct=s\n",
    "            print('End Here')\n",
    "            print(start,s,end)\n",
    "            correct=s\n",
    "\n",
    "        #if start_idx>= start and end>=end_idx: \n",
    "        #    correct=s\n",
    "    # set up a fall back\n",
    "    if correct=='':\n",
    "        sentences = [s for s in default_sentence_split(passage) if answer in s]\n",
    "        correct = random.choice(sentences) if len(sentences)>0 else ''\n",
    "\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130315, 8)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'answer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\isaac.ampomah\\development\\question_generation_trainer\\questions_generation\\notebook\\process_squad_data.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/isaac.ampomah/development/question_generation_trainer/questions_generation/notebook/process_squad_data.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m squad_train\u001b[39m.\u001b[39miterrows():\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/isaac.ampomah/development/question_generation_trainer/questions_generation/notebook/process_squad_data.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     d\u001b[39m.\u001b[39;49manswer\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'answer'"
     ]
    }
   ],
   "source": [
    "for d in squad_train.:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ps',\n",
       " 'The symbol $, usually written before the numerical amount, is used for the U.S. dollar (as well as for many other currencies). The sign was the result of a late 18th-century evolution of the scribal abbreviation \"ps\" for the peso, the common name for the Spanish dollars that were in wide circulation in the New World from the 16th to the 19th centuries. These Spanish pesos or dollars were minted in Spanish America, namely in Mexico City, Potosí, Bolivia; and Lima, Peru. The p and the s eventually came to be written over each other giving rise to $.',\n",
       " 213)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idk= idx\n",
    "squad_train.answer.values[idk],squad_train.context.values[idk],squad_train.answer_start.values[idk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'question', 'is_impossible', 'title', 'context', 'answer',\n",
       "       'answer_start', 'answer_end'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The preludes, many of which are very brief (some consisting of simple statements and developments of a single theme or figure), were described by Schumann as \"the beginnings of studies\".',\n",
       " 'Inspired by J.S.',\n",
       " \"Bach's The Well-Tempered Clavier, Chopin's preludes move up the circle of fifths (rather than Bach's chromatic scale sequence) to create a prelude in each major and minor tonality.\",\n",
       " 'The preludes were perhaps not intended to be played as a group, and may even have been used by him and later pianists as generic preludes to others of his pieces, or even to music by other composers, as Kenneth Hamilton suggests: he has noted a recording by Ferruccio Busoni of 1922, in which the Prelude Op.',\n",
       " '28 No.',\n",
       " '7 is followed by the Étude Op.',\n",
       " '10 No.',\n",
       " '5.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(squad_train.context[idk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The preludes, many of which are very brief (some consisting of simple statements and developments of a single theme or figure), were described by Schumann as \"the beginnings of studies\".',\n",
       " 'Inspired by J.S.',\n",
       " \"Bach's The Well-Tempered Clavier, Chopin's preludes move up the circle of fifths (rather than Bach's chromatic scale sequence) to create a prelude in each major and minor tonality.\",\n",
       " 'The preludes were perhaps not intended to be played as a group, and may even have been used by him and later pianists as generic preludes to others of his pieces, or even to music by other composers, as Kenneth Hamilton suggests: he has noted a recording by Ferruccio Busoni of 1922, in which the Prelude Op. 28 No. 7 is followed by the Étude Op. 10 No. 5.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_sentence_split(squad_train.context[idk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The preludes, many of which are very brief (some consisting of simple statements and developments of a single theme or figure), were described by Schumann as \"the beginnings of studies\".',\n",
       " \"Inspired by J.S. Bach's The Well-Tempered Clavier, Chopin's preludes move up the circle of fifths (rather than Bach's chromatic scale sequence) to create a prelude in each major and minor tonality.\",\n",
       " 'The preludes were perhaps not intended to be played as a group, and may even have been used by him and later pianists as generic preludes to others of his pieces, or even to music by other composers, as Kenneth Hamilton suggests: he has noted a recording by Ferruccio Busoni of 1922, in which the Prelude Op. 28 No. 7 is followed by the Étude Op. 10 No. 5.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_sentence_tokenizer(squad_train.context[idk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "633"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_train.context[idk].index(squad_train.answer[idk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_train.answer[idk] in squad_train.context[idk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sign was the result of a late 18th-century evolution of the scribal abbreviation \"ps\" for the peso, the common name for the Spanish dollars that were in wide circulation in the New World from the 16th to the 19th centuries.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_passage_sentence(squad_train.answer.values[idk],squad_train.context.values[idk],squad_train.answer_start.values[idk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(squad_test.shape[0]):\n",
    "    \n",
    "    oo=extract_passage_sentence(squad_train.answer.values[idk],squad_train.context.values[idk],squad_train.answer_start.values[idk])\n",
    "    if len(oo)<1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11872, (11873, 8))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx,squad_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_sentence_tokenizer(passage):\n",
    "    doc = nlp(passage)\n",
    "    return [s for s in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From September 1823 to 1826 Chopin attended the Warsaw Lyceum , where he received organ lessons from the Czech musician Wilhelm Würfel during his first year .\n",
      "In the autumn of 1826 he began a three - year course under the Silesian composer Józef Elsner at the Warsaw Conservatory , studying music theory , figured bass and composition.[n 3 ]\n",
      "Throughout this period he continued to compose and to give recitals in concerts and salons in Warsaw .\n",
      "He was engaged by the inventors of a mechanical organ , the \" eolomelodicon \" , and on this instrument in May 1825 he performed his own improvisation and part of a concerto by Moscheles .\n",
      "The success of this concert led to an invitation to give a similar recital on the instrument before Tsar Alexander I , who was visiting Warsaw ; the Tsar presented him with a diamond ring .\n",
      "At a subsequent eolomelodicon concert on 10 June 1825 , Chopin performed his Rondo Op . 1 .\n",
      "This was the first of his works to be commercially published and earned him his first mention in the foreign press , when the Leipzig Allgemeine Musikalische Zeitung praised his \" wealth of musical ideas \" .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(squad_train.context[idk])\n",
    "#to print sentences\n",
    "for sent in doc.sents:\n",
    "  print(' '.join([t.text for t in sent]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This was the first of his works to be commercially published and earned him his first mention in the foreign press, when the Leipzig Allgemeine Musikalische Zeitung praised his \"wealth of musical ideas\".'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('From September 1823 to 1826 Chopin attended the Warsaw Lyceum , where he received organ lessons from the Czech musician Wilhelm Würfel during his first year .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('From September 1823 to 1826 Chopin attended the Warsaw Lyceum, where he received organ lessons from the Czech musician Wilhelm Würfel during his first year.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "33487",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\isaac.ampomah\\Anaconda3\\envs\\vsme\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\isaac.ampomah\\Anaconda3\\envs\\vsme\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\isaac.ampomah\\Anaconda3\\envs\\vsme\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2263\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2273\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 33487",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\isaac.ampomah\\development\\question_generation_trainer\\questions_generation\\notebook\\process_squad_data.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/isaac.ampomah/development/question_generation_trainer/questions_generation/notebook/process_squad_data.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m extract_passage_sentence(squad_train\u001b[39m.\u001b[39;49manswer[idk],squad_train\u001b[39m.\u001b[39mcontext[idk],squad_train\u001b[39m.\u001b[39manswer_start[idk])\n",
      "File \u001b[1;32mc:\\Users\\isaac.ampomah\\Anaconda3\\envs\\vsme\\lib\\site-packages\\pandas\\core\\series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    978\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[0;32m    980\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m--> 981\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[0;32m    983\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m    984\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    986\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\isaac.ampomah\\Anaconda3\\envs\\vsme\\lib\\site-packages\\pandas\\core\\series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[0;32m   1088\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1089\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[0;32m   1090\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "File \u001b[1;32mc:\\Users\\isaac.ampomah\\Anaconda3\\envs\\vsme\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 33487"
     ]
    }
   ],
   "source": [
    "extract_passage_sentence(squad_train.answer[idk],squad_train.context[idk],squad_train.answer_start[idk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question</th>\n",
       "      <th>is_impossible</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>False</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>269</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>False</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>207</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "      <td>False</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>2003</td>\n",
       "      <td>526</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "      <td>False</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>166</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "      <td>False</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>late 1990s</td>\n",
       "      <td>276</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           question  \\\n",
       "0           0           When did Beyonce start becoming popular?   \n",
       "1           1  What areas did Beyonce compete in when she was...   \n",
       "2           2  When did Beyonce leave Destiny's Child and bec...   \n",
       "3           3      In what city and state did Beyonce  grow up?    \n",
       "4           4         In which decade did Beyonce become famous?   \n",
       "\n",
       "   is_impossible    title                                            context  \\\n",
       "0          False  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "1          False  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "2          False  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "3          False  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "4          False  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "\n",
       "                answer  answer_start  answer_end  \n",
       "0    in the late 1990s           269          -1  \n",
       "1  singing and dancing           207          -1  \n",
       "2                 2003           526          -1  \n",
       "3       Houston, Texas           166          -1  \n",
       "4           late 1990s           276          -1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_train.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('development')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "473435c5caf2da67d3d84349b3ab99ae605588908510e1f3cdf041055f6c21f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
